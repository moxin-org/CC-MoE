# Collaborative Compression for Large-Scale MoE Deployment on Edge

Authors: Yixiao Chen, Yanyue Xie, Ruining Yang, Pu Zhao *et al.*

[![arXiv](https://img.shields.io/badge/arXiv-2509.25689-df2a2a?logo=arxiv&logoColor=white)](https://www.arxiv.org/abs/2509.25689)
[![HuggingFace](https://img.shields.io/badge/HuggingFace-Moxin-FFD21F?logo=huggingface&logoColor=yellow)](https://huggingface.co/moxin-org/DeepSeek-V3-0324-Moxin-GGUF)
[![DeepSeek](https://img.shields.io/badge/DeepSeek-Reference-0078D7?labelColor=555555&logoColor=white)](https://github.com/deepseek-ai/DeepSeek-V3)
[![llama.cpp](https://img.shields.io/badge/llama.cpp-Reference-4CAF50?labelColor=555555&logo=github&logoColor=white)](https://github.com/ggml-org/llama.cpp)

## Introduction
The Mixture of Experts (MoE) architecture enables scaling Large Language Models (LLMs) efficiently by increasing capacity without raising computation cost. However, ultra-large MoEs like DeepSeek-V3 still pose challenges for deployment on memory-constrained edge devices.  
We introduce a collaborative compression framework that integrates expert pruning, activation adjustment, and mixed-precision quantization, reducing DeepSeek-V3â€™s storage from 1.3 TB to 103 GB while maintaining considerable accuracy. 

## Framework Overview
<div align="center">
  <img src="assets/main.png"/>
</div><br/>
The overall framework architecture of CC-MoE. The middle part shows a block-level schematic of DeepSeek MoE. The left part highlights our Performance-Aware Expert Reduction and Pruning-Aware Activation Adjustment for FFN layers, while the right part illustrates the mixed-precision quantization process applied to the remaining model.

## Usage
For each component of this project, we provide detailed usage instructions and examples in the corresponding subfolder README files.  
Please refer to those for step-by-step tutorials and implementation details.

- [`DeepSeek-V3-Pruning/`](./DeepSeek-V3-Pruning/) â€” Framework for expert pruning and activation adjustment.  
- [`moe-quant/`](./moe-quant/) â€” Mixed-precision quantization with beginner-friendly llama.cpp guidance. 
- [`benchmark/`](./benchmark/) â€” GGUF benchmarking and evaluation methods.  


## Download
All of our released models are publicly available on [ðŸ¤—Hugging Face](https://huggingface.co/moxin-org/DeepSeek-V3-0324-Moxin-GGUF).  
You are welcome to visit our page for more details, or download and test the models directly using our provided scripts.
```
pip install huggingface_hub hf_transfer  # hf_transfer optional: speeds up downloads
python snapshot_download.py
```

---
## Citation

If this work is helpful, please kindly cite as:

```bibtex
@article{chen2025collaborative,
  title={Collaborative Compression for Large-Scale MoE Deployment on Edge},
  author={Chen, Yixiao and Xie, Yanyue and Yang, Ruining and Jiang, Wei and Wang, Wei and He, Yong and Chen, Yue and Zhao, Pu and Wang, Yanzhi},
  journal={arXiv preprint arXiv:2509.25689},
  year={2025}
}
```

## Acknowledgements

This repository builds upon the outstanding work of the following open-source authors and projects:

- [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3).
- [tflsxyy](https://github.com/tflsxyy).
- [ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp), [unsloth.ai](https://unsloth.ai/), [bartowski](https://github.com/bartowski1182).  
- [ikawrakow/ik_llama.cpp](https://github.com/ikawrakow/ik_llama.cpp), [ikawrakow](https://github.com/ikawrakow), [ubergarm](https://github.com/ubergarm).
- [EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness).

We sincerely thank them for their excellent contributions to the open-source community.


